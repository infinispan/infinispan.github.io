---
layout: blog
title: Partitioned clusters tell no lies!
permalink: /blog/:year/:month/:day/partitioned-clusters-tell-no-lies
date: '2014-08-25T02:48:00.001-07:00'
author: Mircea Markus
tags: [ "split brain", "partition handling", "availability" ]
blogger_id: tag:blogger.com,1999:blog-5717179571414330874.post-1678735742300587918
blogger_orig_url: https://blog.infinispan.org/2014/08/partitioned-clusters-tell-no-lies.html
---
== The problem

You are happily running a 10-node cluster. You want failover and speed
and are using distributed mode with 2 copies of data for each key
(numOwners=2). But disaster strikes: a switch in your network crashes
and 5 of your nodes can't reach the other 5 anymore ! Now there are two
independent clusters, each containing 5 nodes, which we are smartly
going to name P1 and P2. Both P1 and P2 continue to serve user requests
(puts and gets) as usual.



This cluster split in two or more parts is called partitioning or split
brain. And it's bad for business, as in really bad ! Bob and Alice share
a bank account stored in the cache. Bob updates his account on P1, then
Alice reads it from P2: she sees a stale value of Bob's account (or even
no value for Bob's account, depending on how the split looks like). This
is a consistency issue, as there's an inconsistent view of the data
between the two partitions.

== Our solution

In Infinispan 7.0.0.Beta1 we added support for reacting to split brains:
if nodes leave, Infinispan acknowledges that data might have been lost
and denies user access to such data. We won't deny access to all the
data, but just the data that might have been affected by the
partitioning. Or, more formally: Infinispan sacrifices data availability
in order to offer consistency (PC
in http://en.wikipedia.org/wiki/CAP_theorem[Brewer's CAP theorem]). For
now partition handling is disabled by default, however we do intend to
make it the default in an upcoming release: running with partition
handling off is like running with scissors: do it at your own risk and
only if you (don't) know what you're doing.



== How we do it

A partition is assumed to happen when *numOwners* or more nodes
disappear at the same time. When this happen two (or more) partitions
form which are not aware of each other. Each such partition does not
start a rebalance, but enters in degraded mode:

* request (read and writes) for entries that have all the copies on
nodes within this partition are honored
* requests for entries that are partially or totally owned by nodes that
disappeared are rejected through an AvailabilityException

To exemplify, consider the initial cluster C0=\{A,B,C,D}, A,B,C,D -
nodes, configured in distributed mode with numOwners=2. Further on, the
cluster contains k1, k2 and k3 keys such that owners(k1) = \{A,B},
owners(k2) = \{B,C} and owners(k3) = \{C,D}. Then a partition happens
C1=\{A,B} and C2=\{C,D}, the degraded mode exhibits the following
behavior:

* on C1, k1 is available for read/write, k2 (partially owned) and k3
(not owned) are not available and accessing them results in an
AvailabilityException
* on C2, k1 and k2  are not available for read/write, k3 is available

A relevant aspect of the partition handling process is the fact that
when a split brain happens, the resulting partitions rely on the
original consistent hash function (the one that existed before the split
brain) in order to calculate key ownership. So it doesn't matter if k1,
k2 or k3 already exists in the cluster or not, as the availability is
strictly determined by the consistent hash and not by the key existence.

If at a further point in time the initial partition C0 forms again as a
result of the network healing and C1 and C2 partitions being merged back
together, then C0 exists the degraded mode becoming fully available
again.



===== Configuration for partition handling functionality

In order to enable partition handling within the XML configuration:





The same can be achieved programmatically:





The actual implementation is work in progress and Beta2 will contain
further improvements which we will publish here!

Cheers,
Mircea Markus

